"""Step Guider for guided tutoring.

This module manages the guiding process for individual steps,
including escape detection, step guidance, and completion evaluation.
"""
import logging
from typing import List, Dict, AsyncGenerator
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from session.models import GuidedStep

logger = logging.getLogger(__name__)


# Escape phrases that trigger direct answer output
ESCAPE_PHRASES = [
    "ç›´æ¥å‘Šè¯‰æˆ‘ç­”æ¡ˆ", "ç›´æ¥ç»™æˆ‘ç­”æ¡ˆ", "æˆ‘ä¸ä¼š", "ä¸ä¼šåš",
    "å‘Šè¯‰æˆ‘å®Œæ•´ç­”æ¡ˆ", "ç›´æ¥è§£ç­”", "è·³è¿‡å¼•å¯¼", "è·³è¿‡",
    "ä¸æƒ³æ€è€ƒäº†", "ç›´æ¥è¯´ç­”æ¡ˆ", "ç»™æˆ‘ç­”æ¡ˆ", "çœ‹ç­”æ¡ˆ",
    "æ”¾å¼ƒ", "å¤ªéš¾äº†", "æƒ³ä¸å‡ºæ¥", "ä¸çŸ¥é“æ€ä¹ˆåš",
    "ç›´æ¥ç»™ç­”æ¡ˆ", "å®Œæ•´ç­”æ¡ˆ", "å…¨éƒ¨ç­”æ¡ˆ"
]


# æ€»ç»“è®²è§£æç¤ºè¯ï¼šä¸“é—¨è´Ÿè´£æ€»ç»“å­¦ç”Ÿå›ç­”å¹¶å®Œæ•´è®²è§£çŸ¥è¯†ç‚¹
SUMMARY_EXPLANATION_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šåˆäº²åˆ‡çš„ç”Ÿç‰©å­¦ç§‘è¾…å¯¼è€å¸ˆï¼Œæ“…é•¿ç”¨ç”ŸåŠ¨æ˜“æ‡‚çš„æ–¹å¼è®²è§£çŸ¥è¯†ç‚¹ã€‚

{persona_prompt}

### ä»»åŠ¡
å­¦ç”Ÿåˆšåˆšå›ç­”äº†ä¸€ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦ï¼š
1. **ç®€è¦æ€»ç»“å­¦ç”Ÿçš„å›ç­”**ï¼ˆ10-20å­—ï¼‰
   - å¦‚æœæ­£ç¡®æˆ–æ¥è¿‘æ­£ç¡®ï¼šç»™äºˆè‚¯å®šï¼Œå¦‚"æ˜¯çš„"ã€"æ²¡é”™"ã€"å¯¹çš„"
   - å¦‚æœé”™è¯¯æˆ–ä¸å®Œæ•´ï¼šæ¸©å’ŒæŒ‡å‡ºï¼Œå¦‚"ä¸å¤ªå‡†ç¡®å‘¢"ã€"è¿˜éœ€è¦è¡¥å……ä¸€ä¸‹"

2. **å®Œæ•´è®²è§£æ­£ç¡®ç­”æ¡ˆ**ï¼ˆ60-100å­—ï¼Œè¿™æ˜¯æ ¸å¿ƒï¼‰
   - ä¸ç®¡å­¦ç”Ÿç­”å¯¹ç­”é”™ï¼Œéƒ½è¦å°†æœ¬æ­¥éª¤çš„æ­£ç¡®ç­”æ¡ˆ/ç»“è®ºå®Œæ•´é™ˆè¿°ä¸€é
   - å¿…é¡»ç»“åˆé¢˜å¹²ä¸­çš„å…·ä½“ä¿¡æ¯ï¼ˆç”Ÿç‰©åç§°ã€æ•°å€¼ã€å®éªŒæ¡ä»¶ç­‰ï¼‰
   - å¿…é¡»ç»“åˆ"æ¶‰åŠçŸ¥è¯†ç‚¹"è¿›è¡Œè¯¦ç»†è§£é‡Š
   - è®²è§£è¦è¯¦ç»†æ¸…æ™°ï¼Œè®©å­¦ç”Ÿå½»åº•ç†è§£è¿™ä¸ªçŸ¥è¯†ç‚¹

### è§£é¢˜ä¸Šä¸‹æ–‡
- åŸé¢˜ç›®ï¼š{question_text}
- æ¶‰åŠçŸ¥è¯†ç‚¹ï¼š{knowledge_points}

### å½“å‰æ­¥éª¤ä¿¡æ¯
- æ­¥éª¤æ ‡é¢˜ï¼š{step_title}
- æ­¥éª¤å†…å®¹ï¼š{step_description}
- æœ¬æ­¥éª¤æ ¸å¿ƒçŸ¥è¯†ç‚¹/æ­£ç¡®ç»“è®ºï¼š{expected_understanding}

### å¯¹è¯è®°å½•
{conversation_history}

### å­¦ç”Ÿæœ€æ–°å›ç­”
"{user_message}"

### è¾“å‡ºç¤ºä¾‹

**ç¤ºä¾‹1ï¼šå­¦ç”Ÿå›ç­”æ­£ç¡®**
å­¦ç”Ÿå›ç­”ï¼š"è·ç¦»è¦ç¦»å¾—æ›´è¿‘"
ä½ çš„è¾“å‡ºï¼š
```
æ˜¯çš„ï¼Œä½ çš„ç†è§£æ˜¯å¯¹çš„å‘¢~ âœ¨

è‹¥è¦è§‚å¯Ÿåˆ°ç»†èƒè¾ƒå¤§ä¸”æ•°é‡è¾ƒå°‘çš„ç‰©åƒï¼Œç‰©é•œç¦»è£…ç‰‡çš„è·ç¦»åº”å½“ç¦»å¾—æ›´è¿‘ã€‚è¿™æ˜¯å› ä¸ºæ˜¾å¾®é•œæ”¾å¤§å€æ•°è¶Šå¤§ï¼Œçœ‹åˆ°çš„ç»†èƒè¶Šå¤§ã€æ•°é‡è¶Šå°‘ï¼Œç‰©é•œç¦»è£…ç‰‡è¶Šè¿‘ï¼Œå…‰åœˆè¶Šå¤§ï¼›æ”¾å¤§å€æ•°è¶Šå°ï¼Œçœ‹åˆ°çš„ç»†èƒè¶Šå°ã€æ•°é‡è¶Šå¤šï¼Œç‰©é•œç¦»è£…ç‰‡è¶Šè¿œï¼Œå…‰åœˆè¶Šå°ã€‚
```

**ç¤ºä¾‹2ï¼šå­¦ç”Ÿå›ç­”ä¸å®Œæ•´**
å­¦ç”Ÿå›ç­”ï¼š"åœ¨å¶å­é‡Œ"
ä½ çš„è¾“å‡ºï¼š
```
ä½ è¯´çš„æ–¹å‘æ˜¯å¯¹çš„ï¼Œä¸è¿‡è¿˜å¯ä»¥æ›´å‡†ç¡®ä¸€äº›å“¦~

å…‰åˆä½œç”¨çš„åœºæ‰€æ˜¯å¶ç»¿ä½“ã€‚å¶ç»¿ä½“æ˜¯æ¤ç‰©ç»†èƒä¸­çš„ä¸€ç§ç»†èƒå™¨ï¼Œä¸»è¦å­˜åœ¨äºå¶ç‰‡çš„å¶è‚‰ç»†èƒä¸­ã€‚å¶ç»¿ä½“å†…å«æœ‰å¶ç»¿ç´ ï¼Œèƒ½å¤Ÿå¸æ”¶å…‰èƒ½ï¼Œå°†äºŒæ°§åŒ–ç¢³å’Œæ°´è½¬åŒ–ä¸ºæœ‰æœºç‰©ï¼Œå¹¶é‡Šæ”¾æ°§æ°”ã€‚è¿™å°±æ˜¯å…‰åˆä½œç”¨çš„å®Œæ•´è¿‡ç¨‹å‘¢ã€‚
```

**ç¤ºä¾‹3ï¼šå­¦ç”Ÿå›ç­”é”™è¯¯**
å­¦ç”Ÿå›ç­”ï¼š"ç»†èƒè†œ"
ä½ çš„è¾“å‡ºï¼š
```
è¿™ä¸ªç­”æ¡ˆä¸å¤ªå‡†ç¡®å‘¢ï¼Œè®©å§å§æ¥å¸®ä½ ç†æ¸…æ¥š~

DNAä¸»è¦å­˜åœ¨äºç»†èƒæ ¸ä¸­ã€‚åœ¨çœŸæ ¸ç»†èƒä¸­ï¼ŒDNAä¸è›‹ç™½è´¨ç»“åˆå½¢æˆæŸ“è‰²ä½“ï¼Œå‚¨å­˜åœ¨ç»†èƒæ ¸å†…ã€‚ç»†èƒæ ¸æ˜¯é—ä¼ ä¿¡æ¯åº“ï¼Œæ§åˆ¶ç€ç»†èƒçš„ç”Ÿå‘½æ´»åŠ¨ã€‚å¦å¤–ï¼Œçº¿ç²’ä½“å’Œå¶ç»¿ä½“ä¸­ä¹Ÿå«æœ‰å°‘é‡DNAï¼Œä½†ç»†èƒæ ¸æ‰æ˜¯DNAçš„ä¸»è¦å­˜åœ¨åœºæ‰€ã€‚
```

### è¾“å‡ºè¦æ±‚
- åªè¾“å‡ºæ€»ç»“å’Œè®²è§£éƒ¨åˆ†ï¼Œä¸è¦æå‡ºæ–°é—®é¢˜
- è¯­è¨€äº²åˆ‡æ´»æ³¼ï¼Œç”¨è¯ç”ŸåŠ¨ä¸ç”Ÿç¡¬
- æ§åˆ¶åœ¨80-120å­—
"""


# å¼•å¯¼é—®é¢˜æç¤ºè¯ï¼šä¸“é—¨è´Ÿè´£ç”Ÿæˆä¸‹ä¸€ä¸ªå¼•å¯¼é—®é¢˜
GUIDING_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šåˆäº²åˆ‡çš„ç”Ÿç‰©å­¦ç§‘è¾…å¯¼è€å¸ˆï¼Œæ“…é•¿ç”¨ç”ŸåŠ¨æ˜“æ‡‚çš„æ–¹å¼ï¼Œå¸¦ç€å­¦ç”Ÿä¸€æ­¥æ­¥æ‹†è§£ç”Ÿç‰©é¢˜ã€åƒé€æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚

{persona_prompt}

### ä»»åŠ¡
å­¦ç”Ÿåˆšåˆšå®Œæˆäº†ä¸€è½®å­¦ä¹ ï¼Œä½ éœ€è¦æ ¹æ®æƒ…å†µå†³å®šä¸‹ä¸€æ­¥ï¼š
- å¦‚æœ{student_reply_count} >= 3ï¼šé¼“åŠ±å­¦ç”Ÿè¿›å…¥ä¸‹ä¸€æ­¥ï¼Œä¸å†æé—®
- å¦‚æœ{student_reply_count} < 3ï¼šæå‡ºä¸€ä¸ªæ–°çš„å¼•å¯¼æ€§é—®é¢˜ï¼Œç»§ç»­æ·±åŒ–ç†è§£

### æ ¸å¿ƒè§„åˆ™
1.  **é—®é¢˜è¦æ±‚**ï¼ˆä»…åœ¨{student_reply_count} < 3æ—¶é€‚ç”¨ï¼‰ï¼š
    - æ¯ä¸ªæ­¥éª¤åªæ**ä¸€ä¸ªå¼•å¯¼æ€§é—®é¢˜**ï¼Œé—®é¢˜å¿…é¡»æ˜ç¡®å¯¹åº”"æ¶‰åŠçŸ¥è¯†ç‚¹"ä¸­çš„æŸä¸€ä¸ªå…·ä½“çŸ¥è¯†ç‚¹
    - **é¿å…é‡å¤çŸ¥è¯†ç‚¹**ï¼šæŸ¥çœ‹å¯¹è¯è®°å½•ï¼Œä¸è¦é‡å¤æé—®ç›¸åŒæˆ–ç›¸ä¼¼çš„çŸ¥è¯†ç‚¹
    - **é—®é¢˜è®¾è®¡åŸåˆ™**ï¼š
      * âŒ ç¦æ­¢ï¼šç›´æ¥é—®"ç­”æ¡ˆæ˜¯ä»€ä¹ˆ""ç»“è®ºæ˜¯ä»€ä¹ˆ""é€‰å“ªä¸ªé€‰é¡¹"
      * âœ… æ­£ç¡®ï¼šé—®çŸ¥è¯†ç‚¹çš„æ¦‚å¿µã€åŸç†ã€å®šä¹‰ã€å…¬å¼ã€é€‚ç”¨æ¡ä»¶
    - é—®é¢˜å¿…é¡»åŒ…å«é¢˜ç›®é‡Œçš„å…·ä½“ä¿¡æ¯ï¼Œä¸¥ç¦ç”¨"è¿™ä¸ª""é‚£ä¸ª""å®ƒ"ç­‰æŒ‡ä»£è¯
    - é—®é¢˜è¦è¯¦å°½ä¸”ç”ŸåŠ¨ï¼Œå¿…é¡»ä»¥ï¼Ÿç»“å°¾

2.  **è¯­æ°”é£æ ¼**ï¼šäº²åˆ‡æ´»æ³¼ï¼Œåƒé¢å¯¹é¢è¾…å¯¼ä¸€æ ·ï¼Œç”¨è¯ç”ŸåŠ¨ä¸ç”Ÿç¡¬

3.  **é•¿åº¦é™åˆ¶**ï¼šæ§åˆ¶åœ¨30-50å­—

### è§£é¢˜ä¸Šä¸‹æ–‡
- åŸé¢˜ç›®ï¼š{question_text}
- æ¶‰åŠçŸ¥è¯†ç‚¹ï¼š{knowledge_points}

### æ‰€æœ‰æ­¥éª¤TODOåˆ—è¡¨
{todo_list}

### å½“å‰æ­¥éª¤ä¿¡æ¯
- æ­¥éª¤åºå·ï¼š{step_index}
- æ­¥éª¤æ ‡é¢˜ï¼š{step_title}
- æ­¥éª¤å†…å®¹ï¼š{step_description}
- æœ¬æ­¥éª¤æ ¸å¿ƒçŸ¥è¯†ç‚¹/æ­£ç¡®ç»“è®ºï¼š{expected_understanding}

### å¯¹è¯è®°å½•
{conversation_history}

### å­¦ç”Ÿæœ€æ–°å›ç­”
"{user_message}"

### å½“å‰è½®æ¬¡
å­¦ç”Ÿå·²å›å¤{student_reply_count}æ¬¡

### è¾“å‡ºè¦æ±‚
- å¦‚æœ{student_reply_count} >= 3ï¼šè¾“å‡ºé¼“åŠ±è¯­ï¼Œå¦‚"å¾ˆå¥½å‘¢ï¼è®©æˆ‘ä»¬ç»§ç»­ä¸‹ä¸€æ­¥å§~ ğŸ’ª"
- å¦‚æœ{student_reply_count} < 3ï¼šè¾“å‡ºä¸€ä¸ªæ–°çš„å¼•å¯¼æ€§é—®é¢˜
- åªè¾“å‡ºé—®é¢˜æˆ–é¼“åŠ±è¯­ï¼Œä¸è¦é‡å¤è®²è§£
- æ§åˆ¶åœ¨30-50å­—
"""


# é‡å†™çš„è¯„ä¼°æç¤ºè¯ï¼šæ”¾å®½åˆ¤æ–­æ ‡å‡†ï¼Œæ„æ€é‡åˆå³å¯
EVALUATION_PROMPT = """ä½ æ˜¯ä¸€ä½å®½å®¹çš„è¾…å¯¼è€å¸ˆï¼Œéœ€è¦åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦åŸºæœ¬ç†è§£å½“å‰æ­¥éª¤çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚

### å½“å‰æ­¥éª¤ä¿¡æ¯
- æ­¥éª¤æ ‡é¢˜ï¼š{step_title}
- æ­¥éª¤å†…å®¹ï¼š{step_description}
- æœ¬æ­¥éª¤æ ¸å¿ƒçŸ¥è¯†ç‚¹/æ­£ç¡®ç»“è®ºï¼š{expected_understanding}

### å¯¹è¯å†å²
{conversation_history}

### å­¦ç”Ÿæœ€æ–°å›å¤
{user_message}

### åˆ¤æ–­æ ‡å‡†ï¼ˆå®½æ¾ï¼‰
1. å­¦ç”Ÿçš„å›ç­”åªè¦ä¸æ ¸å¿ƒçŸ¥è¯†ç‚¹/æ­£ç¡®ç»“è®ºçš„**æ„æ€æœ‰é‡åˆ**å³å¯ï¼Œä¸è¦æ±‚å®Œå…¨å‡†ç¡®æˆ–è¡¨è¿°å®Œæ•´
2. å­¦ç”Ÿæåˆ°äº†å…³é”®æ¦‚å¿µã€å…³é”®æ•°å€¼ã€å…³é”®ç»“è®ºä¸­çš„ä»»ä½•ä¸€ä¸ªï¼Œå°±ç®—ç†è§£
3. å­¦ç”Ÿçš„æ€è·¯æ–¹å‘æ­£ç¡®ï¼Œå³ä½¿ç»†èŠ‚æœ‰è¯¯ï¼Œä¹Ÿç®—åŸºæœ¬æŒæ¡
4. å¦‚æœå¯¹è¯è®°å½•æ˜¾ç¤ºè€å¸ˆå·²ç»ç›´æ¥å‘ŠçŸ¥ç­”æ¡ˆï¼Œä¸”å­¦ç”Ÿè¡¨ç¤ºç†è§£æˆ–è®¤å¯ï¼Œä¹Ÿç®—å®Œæˆ

### è¾“å‡ºè¦æ±‚
ä»…å›å¤"å®Œæˆ"æˆ–"ç»§ç»­"ï¼š
- å­¦ç”Ÿå›ç­”ä¸ç­”æ¡ˆæ„æ€æœ‰é‡åˆ/æ–¹å‘æ­£ç¡®ï¼šå›å¤"å®Œæˆ"
- å­¦ç”Ÿå®Œå…¨ç­”éæ‰€é—®/æ–¹å‘é”™è¯¯ï¼šå›å¤"ç»§ç»­"
"""


class StepGuider:
    """Manages step-by-step guidance in tutoring."""
    
    def __init__(self, model, persona_prompt: str = ""):
        """Initialize with a language model.
        
        Args:
            model: LangChain chat model instance
            persona_prompt: Persona prompt for the tutor
        """
        self.model = model
        self.persona_prompt = persona_prompt
        
        # æ€»ç»“è®²è§£é“¾
        self.summary_prompt = ChatPromptTemplate.from_template(SUMMARY_EXPLANATION_PROMPT)
        self.summary_chain = self.summary_prompt | model
        
        # å¼•å¯¼é—®é¢˜é“¾
        self.guiding_prompt = ChatPromptTemplate.from_template(GUIDING_PROMPT)
        self.guiding_chain = self.guiding_prompt | model
        
        # è¯„ä¼°é“¾
        self.evaluation_prompt = ChatPromptTemplate.from_template(EVALUATION_PROMPT)
        self.evaluation_chain = self.evaluation_prompt | model | StrOutputParser()
    
    def check_escape(self, message: str) -> bool:
        """Check if message contains escape phrases.
        
        Args:
            message: User message
            
        Returns:
            True if escape phrase detected
        """
        message_lower = message.lower().strip()
        for phrase in ESCAPE_PHRASES:
            if phrase in message_lower:
                logger.info(f"ğŸšª [StepGuider] æ£€æµ‹åˆ°è·³å‡ºçŸ­è¯­: {phrase}")
                return True
        return False
    
    async def summarize_and_explain(
        self,
        step: GuidedStep,
        user_message: str,
        conversation_history: List[Dict[str, str]],
        question_text: str = "",
        knowledge_points: List[str] = None
    ) -> AsyncGenerator[str, None]:
        """Summarize student's answer and provide complete explanation.
        
        Args:
            step: Current guided step
            user_message: User's message
            conversation_history: Previous conversation in this step
            question_text: Original question text for context
            knowledge_points: List of knowledge points for this question
            
        Yields:
            Summary and explanation text chunks
        """
        # Format conversation history
        history_str = self._format_history(conversation_history)
        
        # Format knowledge points
        if knowledge_points:
            kp_str = "ã€".join(knowledge_points)
        else:
            kp_str = "ï¼ˆçŸ¥è¯†ç‚¹ä¿¡æ¯æœªæä¾›ï¼‰"
        
        # Build prompt input
        prompt_input = {
            "persona_prompt": self.persona_prompt,
            "question_text": question_text or "ï¼ˆé¢˜ç›®ä¿¡æ¯æœªæä¾›ï¼‰",
            "knowledge_points": kp_str,
            "step_title": step.title,
            "step_description": step.description,
            "expected_understanding": step.expected_understanding,
            "conversation_history": history_str,
            "user_message": user_message
        }
        
        logger.info(f"ğŸ“ [StepGuider] æ€»ç»“è®²è§£æ­¥éª¤ {step.index + 1}: {step.title}")
        
        # Stream summary and explanation
        async for chunk in self.summary_chain.astream(prompt_input):
            if hasattr(chunk, 'content') and chunk.content:
                yield chunk.content
    
    async def generate_next_question(
        self,
        step: GuidedStep,
        user_message: str,
        conversation_history: List[Dict[str, str]],
        question_text: str = "",
        knowledge_points: List[str] = None,
        all_steps: List[GuidedStep] = None,
        student_reply_count: int = 0
    ) -> AsyncGenerator[str, None]:
        """Generate next guiding question or encouragement.
        
        Args:
            step: Current guided step
            user_message: User's message
            conversation_history: Previous conversation in this step
            question_text: Original question text for context
            knowledge_points: List of knowledge points for this question
            all_steps: All guided steps for TODO list display
            student_reply_count: Number of times student has replied in this step
            
        Yields:
            Next question or encouragement text chunks
        """
        # Format conversation history
        history_str = self._format_history(conversation_history)
        
        # Format knowledge points
        if knowledge_points:
            kp_str = "ã€".join(knowledge_points)
        else:
            kp_str = "ï¼ˆçŸ¥è¯†ç‚¹ä¿¡æ¯æœªæä¾›ï¼‰"
        
        # Format TODO list
        if all_steps:
            todo_lines = []
            for s in all_steps:
                checkbox = "â˜‘" if s.completed else "â˜"
                todo_lines.append(f"{checkbox} æ­¥éª¤{s.index + 1}: {s.title}")
            todo_str = "\n".join(todo_lines)
        else:
            todo_str = "ï¼ˆæ­¥éª¤åˆ—è¡¨æœªæä¾›ï¼‰"
        
        # Build prompt input
        prompt_input = {
            "persona_prompt": self.persona_prompt,
            "question_text": question_text or "ï¼ˆé¢˜ç›®ä¿¡æ¯æœªæä¾›ï¼‰",
            "knowledge_points": kp_str,
            "todo_list": todo_str,
            "student_reply_count": student_reply_count,
            "step_index": step.index + 1,
            "step_title": step.title,
            "step_description": step.description,
            "expected_understanding": step.expected_understanding,
            "conversation_history": history_str,
            "user_message": user_message
        }
        
        logger.info(f"â“ [StepGuider] ç”Ÿæˆå¼•å¯¼é—®é¢˜ï¼Œè½®æ¬¡: {student_reply_count}")
        
        # Stream next question or encouragement
        async for chunk in self.guiding_chain.astream(prompt_input):
            if hasattr(chunk, 'content') and chunk.content:
                yield chunk.content
    
    async def guide_step(
        self,
        step: GuidedStep,
        user_message: str,
        conversation_history: List[Dict[str, str]],
        question_text: str = "",
        solution: str = "",
        knowledge_points: List[str] = None,
        all_steps: List[GuidedStep] = None,
        skip_summary: bool = False
    ) -> AsyncGenerator[str, None]:
        """Guide the current step with streaming output (two-stage approach).
        
        This method now uses a two-stage approach:
        1. First, summarize student's answer and provide complete explanation (optional)
        2. Then, generate next guiding question (if needed)
        
        Args:
            step: Current guided step
            user_message: User's message
            conversation_history: Previous conversation in this step
            question_text: Original question text for context
            solution: Complete solution for reference
            knowledge_points: List of knowledge points for this question
            all_steps: All guided steps for TODO list display
            skip_summary: If True, skip the summary/explanation stage (for initial question)
            
        Yields:
            Response text chunks
        """
        # Calculate student reply count in current step
        student_reply_count = sum(1 for msg in conversation_history if msg.get("role") == "user")
        
        logger.info(f"ğŸ¯ [StepGuider] å¼•å¯¼æ­¥éª¤ {step.index + 1}: {step.title}")
        logger.info(f"ğŸ“ [StepGuider] ç”¨æˆ·æ¶ˆæ¯: {user_message}")
        logger.info(f"ğŸ”¢ [StepGuider] å½“å‰æ­¥éª¤å­¦ç”Ÿå›å¤è½®æ¬¡: {student_reply_count}")
        logger.info(f"â­ï¸ [StepGuider] è·³è¿‡æ€»ç»“: {skip_summary}")
        
        # Stage 1: Summarize and explain (only if not skipped)
        if not skip_summary:
            async for chunk in self.summarize_and_explain(
                step, user_message, conversation_history, question_text, knowledge_points
            ):
                yield chunk
            
            # Add spacing between summary and next question
            yield "\n\n"
        
        # Stage 2: Generate next question or encouragement
        async for chunk in self.generate_next_question(
            step, user_message, conversation_history, question_text, 
            knowledge_points, all_steps, student_reply_count
        ):
            yield chunk
    
    async def evaluate_completion(
        self,
        step: GuidedStep,
        user_message: str,
        conversation_history: List[Dict[str, str]]
    ) -> bool:
        """Evaluate if user has completed the current step.
        
        Args:
            step: Current guided step
            user_message: User's latest message
            conversation_history: Previous conversation in this step
            
        Returns:
            True if step is completed
        """
        # Format conversation history
        history_str = self._format_history(conversation_history)
        
        # Build prompt input
        prompt_input = {
            "step_title": step.title,
            "step_description": step.description,
            "expected_understanding": step.expected_understanding,
            "conversation_history": history_str,
            "user_message": user_message
        }
        
        try:
            result = await self.evaluation_chain.ainvoke(prompt_input)
            is_complete = "å®Œæˆ" in result
            logger.info(f"ğŸ“Š [StepGuider] æ­¥éª¤è¯„ä¼°ç»“æœ: {'å®Œæˆ' if is_complete else 'ç»§ç»­'}")
            return is_complete
        except Exception as e:
            logger.error(f"âŒ [StepGuider] è¯„ä¼°å¤±è´¥: {e}")
            return False
    
    def _format_history(self, history: List[Dict[str, str]]) -> str:
        """Format conversation history for prompt.
        
        Args:
            history: List of message dicts with 'role' and 'content'
            
        Returns:
            Formatted history string
        """
        if not history:
            return "ï¼ˆè¿™æ˜¯è¿™ä¸€æ­¥çš„ç¬¬ä¸€æ¬¡å¯¹è¯ï¼‰"
        
        # ä¿ç•™å…¨éƒ¨å†å²ï¼ˆä¸å†æ’é™¤æœ€åä¸€æ¡ï¼Œå› ä¸ºéœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡åˆ¤æ–­å­¦ç”Ÿæ„å›¾ï¼‰
        lines = []
        for msg in history[-6:]:  # ä¿ç•™æœ€è¿‘6æ¡æ¶ˆæ¯ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
            role = "å­¦ç”Ÿ" if msg.get("role") == "user" else "è€å¸ˆ"
            content = msg.get("content", "")[:300]  # æˆªæ–­é•¿æ¶ˆæ¯ï¼Œæ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
            lines.append(f"{role}: {content}")
        
        return "\n".join(lines) if lines else "ï¼ˆè¿™æ˜¯è¿™ä¸€æ­¥çš„ç¬¬ä¸€æ¬¡å¯¹è¯ï¼‰"
