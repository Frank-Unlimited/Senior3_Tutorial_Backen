"""Main Biology Tutor Workflow orchestrating all chains.

This module implements the BiologyTutorWorkflow class that coordinates
all background tasks with proper async execution and SSE notifications.
"""
import asyncio
import logging
from typing import Optional, Any, Dict
from langchain_core.runnables import RunnableParallel

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

from config.settings import Settings
from session.manager import SessionManager
from session.models import TaskStatus, TutoringStyle, ConversationState, Phase2State
from sse.publisher import SSEPublisher
from workflow.model_factory import ModelFactory
from workflow.chains.vision_chain import create_vision_chain
from workflow.chains.solution_chain import create_solution_chain
from workflow.chains.exam_points_chain import create_exam_points_chain
from workflow.chains.knowledge_chain import create_knowledge_chain
from workflow.chains.logic_chain import create_logic_chain
from workflow.phase2_workflow import Phase2Workflow


class BiologyTutorWorkflow:
    """Main workflow orchestrator for biology tutoring."""

    def __init__(
        self,
        settings: Settings,
        session_manager: SessionManager,
        sse_publisher: SSEPublisher
    ):
        """Initialize the workflow.
        
        Args:
            settings: Application settings
            session_manager: Session manager instance
            sse_publisher: SSE publisher instance
        """
        self.settings = settings
        self.session_manager = session_manager
        self.sse = sse_publisher
        
        # Initialize models
        self._init_models()
        
        # Initialize chains
        self._init_chains()

    def _init_models(self) -> None:
        """Initialize AI models from settings."""
        logger.info("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹...")
        logger.info(f"  Vision Model: {self.settings.vision_model.model_name}")
        logger.info(f"  Vision API Base: {self.settings.vision_model.api_base}")
        logger.info(f"  Vision API Key: {self.settings.vision_model.api_key[:10]}...")
        
        self.vision_model = ModelFactory.create_vision_model(
            self.settings.vision_model
        )
        self.deep_model = ModelFactory.create(
            self.settings.deep_thinking_model
        )
        self.quick_model = ModelFactory.create(
            self.settings.quick_model
        )
        logger.info("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

    def _init_chains(self) -> None:
        """Initialize LangChain chains with default models."""
        self.vision_chain = create_vision_chain(self.vision_model)
        self.solution_chain = create_solution_chain(
            self.deep_model,
            self.settings.persona_prompt
        )
        self.exam_points_chain = create_exam_points_chain(self.quick_model)
        self.knowledge_chain = create_knowledge_chain(self.quick_model)
        self.logic_chain_extractor = create_logic_chain(self.quick_model)
        
        # Initialize Phase 2 workflow
        self.phase2_workflow = Phase2Workflow(
            self.settings,
            self.session_manager,
            self.sse,
            self.deep_model
        )
    
    def _get_model_for_session(self, session, model_type: str):
        """Get model for session, using frontend config if available.
        
        Args:
            session: Session object
            model_type: 'vision', 'deep', or 'quick'
            
        Returns:
            Model instance (either from frontend config or default)
        """
        if not session or not session.frontend_model_config:
            # Use default models
            if model_type == 'vision':
                return self.vision_model
            elif model_type == 'deep':
                return self.deep_model
            else:
                return self.quick_model
        
        config = session.frontend_model_config
        logger.info(f"ğŸ”§ [Session] ä½¿ç”¨å‰ç«¯æ¨¡å‹é…ç½®: {model_type}")
        
        # Map model_type to config keys
        model_key = f"{model_type}_model"
        api_key_key = f"{model_type}_api_key"
        
        model_id = config.get(model_key)
        api_key = config.get(api_key_key)
        
        # If no API key provided, use default model
        if not api_key:
            logger.info(f"   æœªæä¾› {model_type} API Keyï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
            if model_type == 'vision':
                return self.vision_model
            elif model_type == 'deep':
                return self.deep_model
            else:
                return self.quick_model
        
        logger.info(f"   Model ID: {model_id}")
        logger.info(f"   API Key: {api_key[:10]}..." if api_key else "   API Key: None")
        
        # Create dynamic model based on model_id
        return self._create_dynamic_model(model_id, api_key, model_type)
    
    def _create_dynamic_model(self, model_id: str, api_key: str, model_type: str):
        """Create a model dynamically based on model ID and API key.
        
        Args:
            model_id: Model identifier (e.g., 'doubao-vision', 'gpt-4')
            api_key: API key for the model
            model_type: 'vision', 'deep', or 'quick'
            
        Returns:
            Model instance
        """
        from langchain_openai import ChatOpenAI
        
        # Determine provider and actual model name based on model_id
        if model_id.startswith('doubao'):
            # Doubao models
            model_map = {
                'doubao-vision': 'doubao-1-5-vision-pro-32k-250115',
                'doubao-pro': 'doubao-1-5-pro-32k-250115',
                'doubao-lite': 'doubao-1-5-lite-32k-250115',
            }
            model_name = model_map.get(model_id, model_id)
            api_base = "https://ark.cn-beijing.volces.com/api/v3"
        elif model_id.startswith('gpt'):
            # OpenAI models
            model_map = {
                'gpt-4': 'gpt-4-turbo-preview',
                'gpt-4-vision': 'gpt-4-vision-preview',
                'gpt-3.5': 'gpt-3.5-turbo',
            }
            model_name = model_map.get(model_id, model_id)
            api_base = "https://api.openai.com/v1"
        elif model_id.startswith('claude'):
            # Anthropic models (via OpenAI-compatible API)
            model_map = {
                'claude-3': 'claude-3-opus-20240229',
                'claude-vision': 'claude-3-opus-20240229',
                'claude-instant': 'claude-instant-1.2',
            }
            model_name = model_map.get(model_id, model_id)
            api_base = "https://api.anthropic.com/v1"
        elif model_id == 'deepseek':
            model_name = "deepseek-chat"
            api_base = "https://api.deepseek.com/v1"
        else:
            # Default to doubao
            model_name = model_id
            api_base = "https://ark.cn-beijing.volces.com/api/v3"
        
        # Set temperature and max_tokens based on model type
        if model_type == 'vision':
            temperature = 0.3
            max_tokens = 2048
        elif model_type == 'deep':
            temperature = 0.7
            max_tokens = 8192
        else:  # quick
            temperature = 0.5
            max_tokens = 1024
        
        logger.info(f"ğŸ¤– [Dynamic] åˆ›å»ºåŠ¨æ€æ¨¡å‹: {model_name}")
        logger.info(f"   API Base: {api_base}")
        logger.info(f"   Temperature: {temperature}")
        
        return ChatOpenAI(
            model=model_name,
            openai_api_key=api_key,
            openai_api_base=api_base,
            temperature=temperature,
            max_tokens=max_tokens,
        )

    async def process_image(
        self,
        session_id: str,
        image_data: bytes,
        mime_type: str = "image/jpeg"
    ) -> None:
        """Process uploaded image - starts background vision extraction.
        
        This method returns immediately after starting the background task.
        
        Args:
            session_id: Session identifier
            image_data: Raw image bytes
            mime_type: Image MIME type
        """
        # Store image in session
        await self.session_manager.update_session(
            session_id,
            image_data=image_data
        )
        
        # Start background vision extraction
        asyncio.create_task(
            self._run_vision_extraction(session_id, image_data, mime_type)
        )

    async def _run_vision_extraction(
        self,
        session_id: str,
        image_data: bytes,
        mime_type: str
    ) -> None:
        """Run vision extraction in background.
        
        Args:
            session_id: Session identifier
            image_data: Raw image bytes
            mime_type: Image MIME type
        """
        logger.info(f"ğŸ–¼ï¸ [Vision] å¼€å§‹è§†è§‰ç†è§£ä»»åŠ¡ session={session_id}")
        logger.info(f"ğŸ–¼ï¸ [Vision] å›¾ç‰‡å¤§å°: {len(image_data)} bytes, MIME: {mime_type}")
        
        # Update task status to running
        await self.session_manager.update_task_status(
            session_id, "vision_extraction", TaskStatus.RUNNING
        )
        
        try:
            # Get session to check for frontend model config
            session = await self.session_manager.get_session(session_id)
            
            # Get vision model (dynamic or default)
            vision_model = self._get_model_for_session(session, 'vision')
            
            # Create vision chain with the model
            vision_chain = create_vision_chain(vision_model)
            
            logger.info("ğŸ–¼ï¸ [Vision] è°ƒç”¨è§†è§‰æ¨¡å‹ API...")
            # Run vision chain
            question_text = await vision_chain.ainvoke({
                "image_data": image_data,
                "mime_type": mime_type
            })
            
            logger.info(f"âœ… [Vision] è§†è§‰ç†è§£æˆåŠŸ! æå–æ–‡æœ¬é•¿åº¦: {len(question_text)}")
            logger.debug(f"âœ… [Vision] æå–å†…å®¹: {question_text[:200]}...")
            
            # Store result in session
            await self.session_manager.update_session(
                session_id,
                question_text=question_text
            )
            
            # Update task status
            await self.session_manager.update_task_status(
                session_id, "vision_extraction", TaskStatus.COMPLETED,
                result=question_text
            )
            
            # Notify frontend
            logger.info("ğŸ“¤ [Vision] å‘é€ SSE é€šçŸ¥...")
            await self.sse.publish_task_completed(
                session_id, "vision_extraction",
                {"question_text": question_text}
            )
            
            # Start parallel analysis tasks
            await self._start_parallel_analysis(session_id, question_text)
            
        except Exception as e:
            logger.error(f"âŒ [Vision] è§†è§‰ç†è§£å¤±è´¥: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é‰´æƒé”™è¯¯
            error_str = str(e).lower()
            error_msg = str(e)
            if "401" in error_str or "unauthorized" in error_str or "authentication" in error_str or "invalid api key" in error_str:
                error_msg = "API é‰´æƒå¤±è´¥ï¼šAPI Key æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·åœ¨è®¾ç½®ä¸­æ£€æŸ¥è§†è§‰æ¨¡å‹çš„ API Key"
            elif "403" in error_str or "forbidden" in error_str:
                error_msg = "API è®¿é—®è¢«æ‹’ç»ï¼šè¯·æ£€æŸ¥ API Key æƒé™æˆ–è´¦æˆ·çŠ¶æ€"
            elif "rate limit" in error_str or "429" in error_str:
                error_msg = "è¯·æ±‚å¤ªé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•"
            
            # Update task status to failed
            await self.session_manager.update_task_status(
                session_id, "vision_extraction", TaskStatus.FAILED,
                error=error_msg
            )
            
            # Notify frontend of failure
            await self.sse.publish_task_failed(
                session_id, "vision_extraction", error_msg
            )

    async def _start_parallel_analysis(
        self,
        session_id: str,
        question_text: str
    ) -> None:
        """Start parallel exam points and solution tasks after vision extraction.
        
        Uses LangChain's RunnableParallel for concurrent execution of:
        - exam_points: Quick summary of exam points (fast)
        - deep_solution: Detailed solution generation (slow)
        
        Both tasks run in parallel based on the extracted question text.
        User input (thinking process, tutoring style) is collected separately
        and will be used in Phase 2 for personalized tutoring.
        
        Args:
            session_id: Session identifier
            question_text: Extracted question text
        """
        logger.info(f"ğŸ”€ [Parallel] å¯åŠ¨å¹¶è¡Œåˆ†æä»»åŠ¡ session={session_id}")
        
        # Get session for context
        session = await self.session_manager.get_session(session_id)
        if not session:
            logger.error(f"âŒ [Parallel] ä¼šè¯ä¸å­˜åœ¨: {session_id}")
            return
        
        # Update both task statuses to RUNNING
        await self.session_manager.update_task_status(
            session_id, "exam_points", TaskStatus.RUNNING
        )
        await self.session_manager.update_task_status(
            session_id, "deep_solution", TaskStatus.RUNNING
        )
        
        # Get models for this session (dynamic or default)
        quick_model = self._get_model_for_session(session, 'quick')
        deep_model = self._get_model_for_session(session, 'deep')
        
        # Create chains with session-specific models
        exam_points_chain = create_exam_points_chain(quick_model)
        solution_chain = create_solution_chain(deep_model, self.settings.persona_prompt)
        
        # Create RunnableParallel for concurrent execution
        parallel_chain = RunnableParallel(
            exam_points=exam_points_chain,
            solution=solution_chain
        )
        
        # Start parallel execution in background
        asyncio.create_task(
            self._run_parallel_analysis(session_id, question_text, parallel_chain)
        )
    
    async def _run_parallel_analysis(
        self,
        session_id: str,
        question_text: str,
        parallel_chain: RunnableParallel
    ) -> None:
        """Execute parallel analysis using RunnableParallel.
        
        This method runs exam_points and solution chains concurrently,
        then handles results and triggers subsequent tasks.
        
        Args:
            session_id: Session identifier
            question_text: Question text
            parallel_chain: RunnableParallel instance
        """
        import time
        start_time = time.time()
        
        logger.info(f"â±ï¸ [Parallel] å¼€å§‹å¹¶è¡Œæ‰§è¡Œ...")
        
        try:
            # Execute both chains in parallel
            results = await parallel_chain.ainvoke({
                "question": question_text
            })
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… [Parallel] å¹¶è¡Œæ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}s")
            
            # Handle exam points result (fast task)
            await self._handle_exam_points_result(session_id, results.get("exam_points", {}))
            
            # Handle solution result (slow task)
            await self._handle_solution_result(session_id, question_text, results.get("solution", ""))
            
        except Exception as e:
            logger.error(f"âŒ [Parallel] å¹¶è¡Œæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é‰´æƒé”™è¯¯
            error_str = str(e).lower()
            error_msg = str(e)
            if "401" in error_str or "unauthorized" in error_str or "authentication" in error_str or "invalid api key" in error_str:
                error_msg = "API é‰´æƒå¤±è´¥ï¼šAPI Key æ— æ•ˆæˆ–å·²è¿‡æœŸï¼Œè¯·åœ¨è®¾ç½®ä¸­æ£€æŸ¥æ¨¡å‹çš„ API Key"
            elif "403" in error_str or "forbidden" in error_str:
                error_msg = "API è®¿é—®è¢«æ‹’ç»ï¼šè¯·æ£€æŸ¥ API Key æƒé™æˆ–è´¦æˆ·çŠ¶æ€"
            elif "rate limit" in error_str or "429" in error_str:
                error_msg = "è¯·æ±‚å¤ªé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•"
            
            # Mark both tasks as failed
            await self.session_manager.update_task_status(
                session_id, "exam_points", TaskStatus.FAILED, error=error_msg
            )
            await self.session_manager.update_task_status(
                session_id, "deep_solution", TaskStatus.FAILED, error=error_msg
            )
            
            # Notify frontend
            await self.sse.publish_task_failed(session_id, "exam_points", error_msg)
            await self.sse.publish_task_failed(session_id, "deep_solution", error_msg)
    
    async def _handle_exam_points_result(
        self,
        session_id: str,
        result: dict
    ) -> None:
        """Handle exam points extraction result.
        
        Args:
            session_id: Session identifier
            result: Exam points result dict
        """
        logger.info(f"ğŸ“Š [ExamPoints] å¤„ç†è€ƒå¯Ÿç‚¹ç»“æœ")
        
        try:
            # Store result in session
            await self.session_manager.update_session(
                session_id,
                exam_points=result.get("exam_points", [])
            )
            
            # Update task status
            await self.session_manager.update_task_status(
                session_id, "exam_points", TaskStatus.COMPLETED,
                result=result
            )
            
            # Notify frontend
            await self.sse.publish_task_completed(
                session_id, "exam_points", result
            )
            
            logger.info(f"âœ… [ExamPoints] è€ƒå¯Ÿç‚¹å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ [ExamPoints] å¤„ç†å¤±è´¥: {e}")
            await self.session_manager.update_task_status(
                session_id, "exam_points", TaskStatus.FAILED, error=str(e)
            )
            await self.sse.publish_task_failed(session_id, "exam_points", str(e))
    
    async def _handle_solution_result(
        self,
        session_id: str,
        question_text: str,
        solution: str
    ) -> None:
        """Handle deep solution generation result.
        
        Args:
            session_id: Session identifier
            question_text: Original question text
            solution: Generated solution
        """
        logger.info(f"ğŸ§  [Solution] å¤„ç†è§£ç­”ç»“æœ")
        
        try:
            # Store result in session
            await self.session_manager.update_session(
                session_id,
                solution=solution
            )
            
            # Update task status
            await self.session_manager.update_task_status(
                session_id, "deep_solution", TaskStatus.COMPLETED,
                result=solution
            )
            
            # Notify frontend
            await self.sse.publish_task_completed(
                session_id, "deep_solution",
                {"solution": solution}
            )
            
            logger.info(f"âœ… [Solution] è§£ç­”å¤„ç†å®Œæˆ")
            
            # Start knowledge and logic extraction
            await self._start_knowledge_extraction(
                session_id, question_text, solution
            )
            
        except Exception as e:
            logger.error(f"âŒ [Solution] å¤„ç†å¤±è´¥: {e}")
            await self.session_manager.update_task_status(
                session_id, "deep_solution", TaskStatus.FAILED, error=str(e)
            )
            await self.sse.publish_task_failed(session_id, "deep_solution", str(e))

    async def _start_knowledge_extraction(
        self,
        session_id: str,
        question_text: str,
        solution: str
    ) -> None:
        """Start parallel knowledge and logic chain extraction using RunnableParallel.
        
        Uses LangChain's RunnableParallel for concurrent execution of:
        - knowledge_points: Knowledge points and common mistakes extraction
        - logic_chain: Solving logic chain extraction
        
        Args:
            session_id: Session identifier
            question_text: Question text
            solution: Generated solution
        """
        logger.info(f"ğŸ”€ [Knowledge] å¯åŠ¨çŸ¥è¯†ç‚¹å’Œé€»è¾‘é“¾å¹¶è¡Œæå– session={session_id}")
        
        # Get session for model config
        session = await self.session_manager.get_session(session_id)
        
        # Update both task statuses to RUNNING
        await self.session_manager.update_task_status(
            session_id, "knowledge_points", TaskStatus.RUNNING
        )
        await self.session_manager.update_task_status(
            session_id, "logic_chain", TaskStatus.RUNNING
        )
        
        # Get quick model for this session (dynamic or default)
        quick_model = self._get_model_for_session(session, 'quick')
        
        # Create chains with session-specific model
        knowledge_chain = create_knowledge_chain(quick_model)
        logic_chain = create_logic_chain(quick_model)
        
        # Create RunnableParallel for concurrent execution
        knowledge_parallel_chain = RunnableParallel(
            knowledge=knowledge_chain,
            logic=logic_chain
        )
        
        # Start parallel execution in background
        asyncio.create_task(
            self._run_knowledge_parallel(
                session_id, question_text, solution, knowledge_parallel_chain
            )
        )
    
    async def _run_knowledge_parallel(
        self,
        session_id: str,
        question_text: str,
        solution: str,
        parallel_chain: RunnableParallel
    ) -> None:
        """Execute parallel knowledge and logic extraction.
        
        Args:
            session_id: Session identifier
            question_text: Question text
            solution: Generated solution
            parallel_chain: RunnableParallel instance
        """
        import time
        start_time = time.time()
        
        logger.info(f"â±ï¸ [Knowledge] å¼€å§‹å¹¶è¡Œæ‰§è¡ŒçŸ¥è¯†ç‚¹å’Œé€»è¾‘é“¾æå–...")
        
        try:
            # Execute both chains in parallel
            results = await parallel_chain.ainvoke({
                "question": question_text,
                "solution": solution
            })
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… [Knowledge] å¹¶è¡Œæ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}s")
            
            # Handle knowledge points result
            await self._handle_knowledge_result(session_id, results.get("knowledge", {}))
            
            # Handle logic chain result
            await self._handle_logic_result(session_id, results.get("logic", {}))
            
            # Check if all tasks complete and mark session as complete
            await self._check_session_complete(session_id)
            
        except Exception as e:
            logger.error(f"âŒ [Knowledge] å¹¶è¡Œæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            
            # Mark both tasks as failed
            await self.session_manager.update_task_status(
                session_id, "knowledge_points", TaskStatus.FAILED, error=str(e)
            )
            await self.session_manager.update_task_status(
                session_id, "logic_chain", TaskStatus.FAILED, error=str(e)
            )
            
            # Notify frontend
            await self.sse.publish_task_failed(session_id, "knowledge_points", str(e))
            await self.sse.publish_task_failed(session_id, "logic_chain", str(e))
    
    async def _handle_knowledge_result(
        self,
        session_id: str,
        result: dict
    ) -> None:
        """Handle knowledge points extraction result.
        
        Args:
            session_id: Session identifier
            result: Knowledge points result dict
        """
        logger.info(f"ğŸ“š [Knowledge] å¤„ç†çŸ¥è¯†ç‚¹ç»“æœ")
        
        try:
            # Store result in session
            # knowledge_points is now a list of strings, not objects
            await self.session_manager.update_session(
                session_id,
                knowledge_points=result.get("knowledge_points", []),
                common_mistakes=[cm.get("mistake", "") for cm in result.get("common_mistakes", [])]
            )
            
            # Update task status
            await self.session_manager.update_task_status(
                session_id, "knowledge_points", TaskStatus.COMPLETED,
                result=result
            )
            
            # Notify frontend
            await self.sse.publish_task_completed(
                session_id, "knowledge_points", result
            )
            
            logger.info(f"âœ… [Knowledge] çŸ¥è¯†ç‚¹å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ [Knowledge] å¤„ç†å¤±è´¥: {e}")
            await self.session_manager.update_task_status(
                session_id, "knowledge_points", TaskStatus.FAILED, error=str(e)
            )
            await self.sse.publish_task_failed(session_id, "knowledge_points", str(e))
    
    async def _handle_logic_result(
        self,
        session_id: str,
        result: dict
    ) -> None:
        """Handle logic chain extraction result.
        
        Args:
            session_id: Session identifier
            result: Logic chain result dict
        """
        logger.info(f"ğŸ”— [Logic] å¤„ç†é€»è¾‘é“¾ç»“æœ")
        
        try:
            # Store result in session
            await self.session_manager.update_session(
                session_id,
                logic_chain_steps=result.get("steps", []),
                thinking_pattern=result.get("thinking_pattern", "")
            )
            
            # Update task status
            await self.session_manager.update_task_status(
                session_id, "logic_chain", TaskStatus.COMPLETED,
                result=result
            )
            
            # Notify frontend
            await self.sse.publish_task_completed(
                session_id, "logic_chain", result
            )
            
            logger.info(f"âœ… [Logic] é€»è¾‘é“¾å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ [Logic] å¤„ç†å¤±è´¥: {e}")
            await self.session_manager.update_task_status(
                session_id, "logic_chain", TaskStatus.FAILED, error=str(e)
            )
            await self.sse.publish_task_failed(session_id, "logic_chain", str(e))

    async def _check_session_complete(self, session_id: str) -> None:
        """Check if all background tasks are complete and notify.
        
        Args:
            session_id: Session identifier
        """
        session = await self.session_manager.get_session(session_id)
        if session and session.is_all_tasks_completed():
            await self.sse.publish_session_complete(session_id)
            
            # Check if Phase 1 is fully complete (tasks + user input)
            await self._check_phase1_complete(session_id)
    
    async def _check_phase1_complete(self, session_id: str) -> None:
        """Check if Phase 1 data collection is complete.
        
        Phase 1 requires:
        - All background tasks completed (vision, exam_points, solution, knowledge, logic)
        - User thinking process collected
        - User tutoring style selected
        
        Args:
            session_id: Session identifier
        """
        session = await self.session_manager.get_session(session_id)
        if not session:
            return
        
        # Check all 8 data points
        has_all_tasks = session.is_all_tasks_completed()
        has_user_thinking = session.user_thinking is not None
        has_tutoring_style = session.tutoring_style is not None
        
        if has_all_tasks and has_user_thinking and has_tutoring_style:
            # Phase 1 complete - log summary
            self._log_phase1_summary(session)
    
    def _log_phase1_summary(self, session: Any) -> None:
        """Log summary of all collected data at end of phase 1.
        
        Phase 1 collects:
        1. é¢˜å¹² (question_text)
        2. è¯¦ç»†è§£ç­”è¿‡ç¨‹ (solution)
        3. å¿«é€Ÿè€ƒå¯Ÿç‚¹æ€»ç»“ (exam_points)
        4. çŸ¥è¯†ç‚¹åˆ—è¡¨ (knowledge_points)
        5. æ˜“é”™ç‚¹åˆ—è¡¨ (common_mistakes)
        6. è§£é¢˜é€»è¾‘é“¾æ¢³ç† (logic_chain)
        7. ç”¨æˆ·çš„æ€è€ƒè¿‡ç¨‹ (user_thinking)
        8. ç”¨æˆ·å¸Œæœ›çš„è¾…å¯¼æ–¹å¼ (tutoring_style)
        """
        logger.info("=" * 60)
        logger.info("ğŸ“‹ [Phase 1 Complete] ç¬¬ä¸€é˜¶æ®µæ•°æ®æ”¶é›†å®Œæˆ")
        logger.info("=" * 60)
        
        # 1. é¢˜å¹²
        logger.info(f"ğŸ“ 1. é¢˜å¹² (question_text):")
        logger.info(f"   {session.question_text[:200] if session.question_text else 'æœªè·å–'}...")
        
        # 2. è¯¦ç»†è§£ç­”è¿‡ç¨‹
        logger.info(f"âœ… 2. è¯¦ç»†è§£ç­”è¿‡ç¨‹ (solution):")
        logger.info(f"   {session.solution[:200] if session.solution else 'æœªç”Ÿæˆ'}...")
        
        # 3. å¿«é€Ÿè€ƒå¯Ÿç‚¹æ€»ç»“
        logger.info(f"ğŸ“Š 3. å¿«é€Ÿè€ƒå¯Ÿç‚¹æ€»ç»“ (exam_points):")
        if session.exam_points:
            for i, point in enumerate(session.exam_points[:5], 1):
                logger.info(f"   {i}. {point}")
        else:
            logger.info("   æœªè·å–")
        
        # 4. çŸ¥è¯†ç‚¹åˆ—è¡¨
        logger.info(f"ğŸ“š 4. çŸ¥è¯†ç‚¹åˆ—è¡¨ (knowledge_points):")
        if session.knowledge_points:
            for i, kp in enumerate(session.knowledge_points[:5], 1):
                logger.info(f"   {i}. {kp}")
        else:
            logger.info("   æœªè·å–")
        
        # 5. æ˜“é”™ç‚¹åˆ—è¡¨
        logger.info(f"âš ï¸ 5. æ˜“é”™ç‚¹åˆ—è¡¨ (common_mistakes):")
        if session.common_mistakes:
            for i, cm in enumerate(session.common_mistakes[:5], 1):
                logger.info(f"   {i}. {cm}")
        else:
            logger.info("   æœªè·å–")
        
        # 6. è§£é¢˜é€»è¾‘é“¾æ¢³ç†
        logger.info(f"ğŸ”— 6. è§£é¢˜æ­¥éª¤ (logic_chain_steps):")
        if session.logic_chain_steps:
            for i, step in enumerate(session.logic_chain_steps, 1):
                logger.info(f"   {i}. {step}")
        else:
            logger.info("   æœªè·å–")
        
        if session.thinking_pattern:
            logger.info(f"   æ€ç»´æ¨¡å¼: {session.thinking_pattern}")
        
        # 7. ç”¨æˆ·çš„æ€è€ƒè¿‡ç¨‹
        logger.info(f"ğŸ’­ 7. ç”¨æˆ·çš„æ€è€ƒè¿‡ç¨‹ (user_thinking):")
        logger.info(f"   {session.user_thinking if session.user_thinking else 'ç”¨æˆ·æœªæä¾›'}")
        
        # 8. ç”¨æˆ·å¸Œæœ›çš„è¾…å¯¼æ–¹å¼
        logger.info(f"ğŸ¯ 8. ç”¨æˆ·å¸Œæœ›çš„è¾…å¯¼æ–¹å¼ (tutoring_style):")
        style_name = session.tutoring_style.value if session.tutoring_style else 'æœªé€‰æ‹©'
        style_desc = "å¼•å¯¼å¼è¾…å¯¼" if style_name == "guided" else "ç›´æ¥è§£ç­”" if style_name == "direct" else "æœªé€‰æ‹©"
        logger.info(f"   {style_name} ({style_desc})")
        
        # Task status summary
        logger.info("-" * 60)
        logger.info("ğŸ“ˆ ä»»åŠ¡çŠ¶æ€æ±‡æ€»:")
        for task_name, task_state in session.tasks.items():
            status_emoji = "âœ…" if task_state.status.value == "completed" else "âŒ" if task_state.status.value == "failed" else "â³"
            logger.info(f"   {status_emoji} {task_name}: {task_state.status.value}")
        
        logger.info("=" * 60)

    async def process_message(
        self,
        session_id: str,
        message: str
    ) -> str:
        """Process user message based on conversation state.
        
        Args:
            session_id: Session identifier
            message: User message content
            
        Returns:
            AI response message
        """
        session = await self.session_manager.get_session(session_id)
        if not session:
            return "æŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°ä½ çš„ä¼šè¯å‘¢ï¼Œè¯·é‡æ–°å¼€å§‹å§~"
        
        # Add user message to history
        await self.session_manager.add_message(session_id, "user", message)
        
        # Handle based on conversation state
        state = session.conversation_state
        
        if state == ConversationState.INITIAL:
            # First message after image upload - ask about thinking
            # This handles both empty trigger message and actual user message
            response = self._generate_thinking_prompt()
            await self.session_manager.set_conversation_state(
                session_id, ConversationState.AWAITING_THINKING
            )
            
        elif state == ConversationState.AWAITING_THINKING:
            # Store thinking and ask about tutoring style
            await self.session_manager.update_session(
                session_id,
                user_thinking=message
            )
            response = self._generate_style_prompt()
            await self.session_manager.set_conversation_state(
                session_id, ConversationState.AWAITING_STYLE
            )
            
        elif state == ConversationState.AWAITING_STYLE:
            # Parse tutoring style choice and save it
            style = self._parse_tutoring_style(message)
            await self.session_manager.update_session(
                session_id,
                tutoring_style=style
            )
            
            logger.info(f"ğŸ¯ [Workflow] ç”¨æˆ·é€‰æ‹©è¾…å¯¼æ–¹å¼: {style.value}")
            
            # Update conversation state to TUTORING
            await self.session_manager.set_conversation_state(
                session_id, ConversationState.TUTORING
            )
            
            # Return empty - actual response will come from streaming
            response = ""
            
        elif state == ConversationState.TUTORING:
            # Phase 2 tutoring - handled by streaming endpoint
            response = ""
            
        else:
            # General tutoring conversation
            response = "å¥½çš„ï¼Œè®©æˆ‘æ¥å¸®ä½ è§£ç­”è¿™ä¸ªé—®é¢˜å‘¢~"
        
        # Add AI response to history
        await self.session_manager.add_message(session_id, "assistant", response)
        
        return response

    def _generate_thinking_prompt(self) -> str:
        """Generate prompt asking about user's thinking."""
        return (
            "èƒ½å‘Šè¯‰å§å§ä½ æ˜¯æ€ä¹ˆæ€è€ƒè¿™é“é¢˜çš„å—ï¼Ÿ"
            "æœ‰ä»€ä¹ˆåœ°æ–¹è®©ä½ æ„Ÿåˆ°å›°æƒ‘å‘¢ï¼Ÿ\n\n"
            "ä¸ç”¨æ‹…å¿ƒè¯´é”™å“¦ï¼ŒæŠŠä½ çš„æƒ³æ³•å‘Šè¯‰æˆ‘å°±å¥½~ ğŸ’•"
        )

    def _generate_style_prompt(self) -> str:
        """Generate prompt asking about tutoring style."""
        return (
            "å—¯å—¯ï¼Œæˆ‘æ˜ç™½ä½ çš„æƒ³æ³•äº†å‘¢~ ğŸ‘\n\n"
            "é‚£å§å§æƒ³é—®ä¸€ä¸‹ï¼Œä½ å¸Œæœ›æˆ‘æ€ä¹ˆå¸®ä½ å‘¢ï¼Ÿ\n\n"
            "1ï¸âƒ£ **å¼•å¯¼å¼è¾…å¯¼** - æˆ‘ä¼šä¸€æ­¥ä¸€æ­¥å¼•å¯¼ä½ æ€è€ƒï¼Œå¸®ä½ è‡ªå·±æ‰¾åˆ°ç­”æ¡ˆ\n"
            "2ï¸âƒ£ **ç›´æ¥è§£ç­”** - æˆ‘ä¼šç›´æ¥ç»™ä½ è¯¦ç»†çš„è§£ç­”è¿‡ç¨‹\n\n"
            "å›å¤ 1 æˆ– 2 å‘Šè¯‰æˆ‘å§~"
        )

    def _parse_tutoring_style(self, message: str) -> TutoringStyle:
        """Parse tutoring style from user message."""
        message = message.strip().lower()
        if "1" in message or "å¼•å¯¼" in message:
            return TutoringStyle.GUIDED
        return TutoringStyle.DIRECT

    def _generate_tutoring_start_message(self, style: TutoringStyle) -> str:
        """Generate message when tutoring starts."""
        if style == TutoringStyle.GUIDED:
            return (
                "å¥½çš„ï¼Œé‚£æˆ‘ä»¬ä¸€èµ·æ¥æ¢ç´¢è¿™é“é¢˜å§~ ğŸŒŸ\n\n"
                "æˆ‘ä¼šä¸€æ­¥ä¸€æ­¥å¼•å¯¼ä½ ï¼Œä¸ç”¨ç€æ€¥ï¼Œæ…¢æ…¢æ¥å“¦~\n"
                "å¦‚æœåå°åˆ†æå®Œæˆäº†ï¼Œæˆ‘ä¼šé©¬ä¸Šå‘Šè¯‰ä½ çš„ï¼"
            )
        else:
            return (
                "å¥½çš„ï¼Œé‚£æˆ‘æ¥ç»™ä½ è¯¦ç»†è®²è§£è¿™é“é¢˜~ ğŸ“\n\n"
                "è¯·ç¨ç­‰ä¸€ä¸‹ï¼Œæˆ‘æ­£åœ¨æ•´ç†è§£ç­”è¿‡ç¨‹...\n"
                "åˆ†æå®Œæˆåä¼šç«‹åˆ»å‘Šè¯‰ä½ å“¦ï¼"
            )

    async def process_phase2_message_stream(
        self,
        session_id: str,
        message: str
    ):
        """Process Phase 2 message with streaming output.
        
        This method handles tutoring after user selects their preferred style.
        It delegates to Phase2Workflow for actual processing.
        
        Args:
            session_id: Session identifier
            message: User message
            
        Yields:
            Response text chunks
        """
        session = await self.session_manager.get_session(session_id)
        if not session:
            yield "æŠ±æ­‰ï¼Œæ‰¾ä¸åˆ°ä½ çš„ä¼šè¯å‘¢ï¼Œè¯·é‡æ–°å¼€å§‹å§~"
            return
        
        # Get model for this session
        deep_model = self._get_model_for_session(session, 'deep')
        
        # Create Phase2Workflow with session-specific model
        phase2 = Phase2Workflow(
            self.settings,
            self.session_manager,
            self.sse,
            deep_model
        )
        
        # Delegate to Phase2Workflow
        async for chunk in phase2.process_message_stream(session_id, message):
            yield chunk
